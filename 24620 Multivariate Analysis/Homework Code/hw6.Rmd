---
title: "hw6"
author: "Bin Yu"
date: "2025-05-11"
output: html_document
---
```{r}
# Distance matrix between chapters
dmat <- matrix(
  c(
    0.00, 0.76, 2.97, 4.88, 3.86,
    0.76, 0.00, 0.80, 4.17, 1.96,
    2.97, 0.80, 0.00, 0.21, 1.51,
    4.88, 4.17, 0.21, 0.00, 0.51,
    3.86, 1.96, 1.51, 0.51, 0.00
  ),
  nrow = 5,
  byrow = TRUE,
  dimnames = list(
    paste0("Ch", 1:5),
    paste0("Ch", 1:5)
  )
)

# Convert to 'dist' object
d <- as.dist(dmat)

# Perform hierarchical clustering with three linkage methods
hc_single   <- hclust(d, method = "single")
hc_complete <- hclust(d, method = "complete")
hc_average  <- hclust(d, method = "average")

# Plot dendrograms (optional)
par(mfrow = c(1,3))
plot(hc_single,   main = "Single Linkage",   sub = "", xlab = "")
plot(hc_complete, main = "Complete Linkage", sub = "", xlab = "")
plot(hc_average,  main = "Average Linkage",  sub = "", xlab = "")

# Cut each tree into k = 3 clusters
clust_single   <- cutree(hc_single,   k = 3)
clust_complete <- cutree(hc_complete, k = 3)
clust_average  <- cutree(hc_average,  k = 3)

# Print cluster assignments
cat("Clusters (k=3) - Single Linkage:\n")
print(split(names(clust_single),   clust_single))
cat("\nClusters (k=3) - Complete Linkage:\n")
print(split(names(clust_complete), clust_complete))
cat("\nClusters (k=3) - Average Linkage:\n")
print(split(names(clust_average),  clust_average))

```
```{r}
rm(list=ls())
ladyrun = read.table("/Users/yubin/Desktop/Multivariate Analysis/ladyrun25.dat")
colnames(ladyrun)=c("Country","100m","200m","400m","800m","1500m","3000m","Marathon")

X <- ladyrun[, 2:8]
NormX <- as.matrix(X) %*% solve(diag(sqrt(diag(var(X)))))

distobs <- dist(NormX, method = "euclidean")

# 6. Convert to full matrix and mask self‐distances
dm <- as.matrix(distobs)
diag(dm) <- NA

# 7. Find indices of the maximum and minimum distances
max_idx <- which(dm == max(dm, na.rm = TRUE), arr.ind = TRUE)[1, ]
min_idx <- which(dm == min(dm, na.rm = TRUE), arr.ind = TRUE)[1, ]

# 8. Retrieve country names and distance values
countries <- ladyrun$Country
max_pair  <- countries[max_idx]
min_pair  <- countries[min_idx]
max_dist  <- dm[max_idx[1], max_idx[2]]
min_dist  <- dm[min_idx[1], min_idx[2]]

# 9. Print results
cat("Maximum Euclidean distance =", round(max_dist, 3),
    "between", max_pair[1], "and", max_pair[2], "\n")
cat("Minimum Euclidean distance =", round(min_dist, 3),
    "between", min_pair[1], "and", min_pair[2], "\n")
```
```{r}
NormX <- as.matrix(X) %*% solve(diag(sqrt(diag(var(X)))))
dmat = dist(NormX); # 54 by 54
Dmat = as.matrix(dmat)


rownames(Dmat)=ladyrun[,1]; colnames(Dmat)=ladyrun[,1]
heatmap(as.matrix(dmat),symm=T,col=gray.colors(100),Rowv=NA,Colv = "Rowv")
max(Dmat); which(Dmat == max(Dmat), arr.ind = TRUE)
min(Dmat[Dmat>0]); which(Dmat == min(Dmat[Dmat>0]), arr.ind = TRUE)
```


```{r}
# 4. Hierarchical clustering with complete linkage
hc_complete <- hclust(distobs, method = "complete")

# 5. Cut the tree into k = 8 clusters
k        <- 8
clust8   <- cutree(hc_complete, k = k)

# 6. Compute cluster sizes
sizes    <- table(clust8)

# 7. Identify the three smallest clusters
smallest_ids    <- names(sort(sizes))[1:3]

# 8. Extract country abbreviations in those clusters
clusters_small  <- split(ladyrun$Country, clust8)[smallest_ids]

# 9. Print the three smallest clusters
print(clusters_small)
```
```{r}
# 4. Hierarchical clustering with complete linkage
hc_complete <- hclust(distobs, method = "complete")

# 5. Cut the tree into k = 8 clusters
k        <- 7
clust7   <- cutree(hc_complete, k = k)

# 6. Compute cluster sizes
sizes    <- table(clust7)

# 7. Identify the three smallest clusters
smallest_ids    <- names(sort(sizes))[1:3]

# 8. Extract country abbreviations in those clusters
clusters_small  <- split(ladyrun$Country, clust7)[smallest_ids]

# 9. Print the three smallest clusters
print(clusters_small)
```
```{r}

# 3. Grand‐mean x̄ and total sum of squares (denominator)
xbar <- colMeans(X)
totss <- sum(rowSums((X - matrix(xbar, nrow = nrow(X),
                                  ncol = ncol(X), byrow = TRUE))^2))

set.seed(42)
Kvals <- 2:8
PK    <- numeric(length(Kvals))

for (i in seq_along(Kvals)) {
  K  <- Kvals[i]
  km <- kmeans(X, centers = K, nstart = 25)
  
  # numerator = ∑ within‐cluster squared distances
  # kmeans reports this as tot.withinss
  wss <- km$tot.withinss
  
  PK[i] <- 1 - wss / totss
}

# 5. Display results
results <- data.frame(K = Kvals, P_K = round(PK, 4))
print(results)

# 6. (Optional) Plot P_K vs. K
plot(results$K, results$P_K, type = "b", pch = 19, lwd = 2,
     xlab = "Number of clusters K",
     ylab = expression(P[K]),
     main = expression("Proportion of SS Explained" ~ P[K]))
```

```{r}
df_clusters <- data.frame(
  Country = ladyrun$Country,
  Cluster = clust8
)
print(df_clusters)
```

```{r}
# Define the data
df <- data.frame(
  x1 = c(5,  1, -1, 3),
  x2 = c(-4, -2,  1, 1),
  row.names = c("A", "B", "C", "D")
)

# Run K‐means with multiple random starts
set.seed(123)
km <- kmeans(df,2)

# 1. Final cluster assignments
print(km$cluster)

# 2. Cluster centroids
print(km$centers)

# 3. Squared distances of each point to its assigned centroid
dist2 <- rowSums((df - km$centers[km$cluster, ])^2)
print(dist2)

```

```{r}
# 定义分量密度
f1 <- function(x) ifelse(x>=0 & x<=1, 2*x, 0)
f2 <- function(x) ifelse(x>=0 & x<=1, 2*(1-x), 0)

# 三种样本
xs_a <- c(0.1, 0.2, 0.3, 0.4, 0.7)
xs_b <- c(0.1, 0.2, 0.3, 0.4, 0.9)
xs_c <- c(0.1, 0.2, 0.3, 0.6, 0.9)

# p1 网格
p1 <- seq(0, 1, length.out = 1000)

# 计算对数似然
logL <- function(xs, p1) {
  sapply(p1, function(p) {
    sum(log(p * f1(xs) + (1-p) * f2(xs)))
  })
}

ll_a <- logL(xs_a, p1)
ll_b <- logL(xs_b, p1)
ll_c <- logL(xs_c, p1)

# 绘图
plot(p1, ll_a, type = "l", lwd = 2, col = "blue",
     xlab = expression(p[1]), ylab = "Log-Likelihood",
     main = "Log-Likelihood vs p1 for Cases (a),(b),(c)")
lines(p1, ll_b, col = "red",   lwd = 2, lty = 2)
lines(p1, ll_c, col = "darkgreen", lwd = 2, lty = 3)
legend("topright", legend = c("Case (a)", "Case (b)", "Case (c)"),
       col = c("blue", "red", "darkgreen"), lwd = 2, lty = c(1,2,3))

```

```{r}
X <- matrix(c(
  3, 6, 0,
  4, 4, 3,
  NA, 8, 3,
  5, 6, 2
), nrow = 4, byrow = TRUE)
colnames(X) <- paste0("X", 1:3)
rownames(X) <- paste0("obs", 1:4)

col_means <- colMeans(X, na.rm = TRUE)
X_imp <- X
for(j in seq_len(ncol(X_imp))) {
  X_imp[is.na(X_imp[, j]), j] <- col_means[j]
}

n <- nrow(X_imp)
mu_hat  <- colMeans(X_imp)                    
Sigma_hat <- cov(X_imp) * (n - 1) / n        

mu1   <- mu_hat[1]
mu23  <- mu_hat[2:3]
S11   <- Sigma_hat[1, 1]
S12   <- Sigma_hat[1, 2:3]
S21   <- Sigma_hat[2:3, 1]
S22   <- Sigma_hat[2:3, 2:3]
obs3_23 <- X[3, 2:3]                         # (8,3)
x31_new <- mu1 + S12 %*% solve(S22) %*% (obs3_23 - mu23)
X_imp[3, 1] <- x31_new
print(x31_new)

mu_hat1   <- colMeans(X_imp)
Sigma_hat1 <- cov(X_imp) * (n - 1) / n

print(mu_hat1)
print(Sigma_hat1)
```


```{r}
mu1_1   <- mu_hat1[1]
mu23_1  <- mu_hat1[2:3]
S11_1   <- Sigma_hat1[1, 1]
S21_1   <- Sigma_hat1[2:3, 1]
obs4_1 <- X[4, 1]                            
x23_new_obs4 <- mu23_1 + S21_1 * (1/S11_1) * (obs4_1 - mu1_1)
X_imp[4, 2:3] <- as.numeric(x23_new_obs4)

print(x23_new_obs4)

mu_hat2    <- colMeans(X_imp)
Sigma_hat2 <- cov(X_imp) * (n - 1) / n

print(mu_hat2)
print(Sigma_hat2)
```

